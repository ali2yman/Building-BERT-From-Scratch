{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XaFrUMjvPO"
      },
      "source": [
        "# BERT From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "-aiVOwuxjvPP"
      },
      "source": [
        "### 1- Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-8kZmr4ItGUj"
      },
      "outputs": [],
      "source": [
        "import math # Mathematics\n",
        "import re  # Regular expersions\n",
        "from random import *  # Random values\n",
        "import numpy as np  # Matrices\n",
        "import torch  # Transformer # TF # JAX >>> google package Enable us to build BERT too\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  # Optimizers\n",
        "from transformers import BertTokenizer, BertModel, AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "a4FLUBOAjvPQ"
      },
      "source": [
        "### 2- Creating - **Loading** Our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w6YMNvc8tbA9"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "        'Hello, how are you? I am Ali.\\n'\n",
        "        'Hello, Ali My name is wael. Nice to meet you.\\n'\n",
        "        'Nice meet you too. How are you today?\\n'\n",
        "        'Great. My baseball team won the competition.\\n'\n",
        "        'Oh Congratulations, wael\\n'\n",
        "        'Thank you Ali'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "D4oETOZ_jvPR"
      },
      "source": [
        "## 3- Data Preprocessing\n",
        "\n",
        "1. Clean Data from .,?!\n",
        "2. Encode our data (ids).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AhX8b1ydtrVf"
      },
      "outputs": [],
      "source": [
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
        "\n",
        "word_list = list(set(\" \".join(sentences).split())) # we make a join for all sentence to be as one sentense then split by \" \" and make it in set to remove redundant words the in a list\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4          # For Encoding start from 4\n",
        "\n",
        "number_dict = {i: w for i, w in enumerate(word_dict)} # Create a reverse dictionary for decoding\n",
        "vocab_size = len(word_dict)\n",
        "\n",
        "token_list = list()\n",
        "# Tokenize each sentence into a list of token IDs\n",
        "for sentence in sentences:\n",
        "    arr = [word_dict[s] for s in sentence.split()]\n",
        "    # print(\"arr = \", arr)\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "AI07u_PCjvPR"
      },
      "source": [
        "### 4. Prepare our Data for BERT.\n",
        "1. Configure BERT Parameters.\n",
        "2. Make Batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q03SGkfIu_Kd"
      },
      "outputs": [],
      "source": [
        "batch_size = 6\n",
        "maxlen = 30 # maximum of length\n",
        "max_pred = 5  # max tokens of prediction\n",
        "n_layers = 6 # number of Encoder of Encoder Layer\n",
        "n_heads = 12 # number of heads in Multi-Head Attention\n",
        "d_model = 768 # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V        64*12 head = 768\n",
        "n_segments = 2 # n.o.sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TtyOOmRntu8w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))  # random index , random index\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]  # random sentence , random sentence\n",
        "\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']] # concatenate random sentences with 'CLS' and 'SEP'\n",
        "\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)  # [ 0,0,0,0,0,0,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "        #MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # number of masks is up to 15 % of tokens in one sentence\n",
        "\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]  # list of indices of input without cls and sep tokens\n",
        "\n",
        "        shuffle(cand_maked_pos)    # shuffle all indices\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)     # list of random indices\n",
        "            masked_tokens.append(input_ids[pos]) # list of random tokens\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
        "            elif random() < 0.5:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word_dict[number_dict[index]] # replace by random word\n",
        "\n",
        "        # Zero Paddings\n",
        "        n_pad = maxlen - len(input_ids) # number of pad tokens we need\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "    #     # Zero Padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred: # target 1 , max = 5   [12,0,0,0,0 ]\n",
        "            n_pad = max_pred - n_pred # number of pad tokens in masked list\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)  # to make masked list is static in shape\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7Cstf-GGjvPS"
      },
      "source": [
        "### 5. Core Functions\n",
        "1. Get Attention Pad Mask.\n",
        "2. GELU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token / searching for pad tokens\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lgJwW4OaiXE2"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q7_HC-Y0jC3K"
      },
      "outputs": [],
      "source": [
        "batch = make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XM1-FdPJi6p3"
      },
      "outputs": [],
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch)) # convert to torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhM1DCU_iYCB",
        "outputId": "56c21a6a-1c55-4050-9b90-80228902e1d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True]) tensor([ 1, 21,  3,  5, 22,  3, 10,  5,  6,  2, 11, 16,  3, 25, 12, 15, 14,  2,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "KcbssPoRjvPU"
      },
      "source": [
        "### 6. Building Blocks\n",
        "1. Embedding.\n",
        "2. Multi Head Attention.\n",
        "3. Feed Forward\n",
        "4. Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qnay0LTDjE4S"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding # size of dictionary, embed vector size\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rHjj-1wXjsdI"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X2rbGNMzl7o",
        "outputId": "246be666-c230-49f6-a341-da66f248048f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masks tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
            "\n",
            "nnAttention Scores after softmax:  tensor([1.0000e+00, 6.9790e-27, 3.9032e-30, 2.9727e-28, 2.1375e-27, 7.5038e-30,\n",
            "        3.5430e-27, 2.4213e-28, 7.2006e-29, 1.0190e-26, 7.7632e-43, 1.4781e-41,\n",
            "        1.4013e-45, 8.7301e-43, 4.2039e-45, 2.9427e-44, 4.5682e-43, 1.5947e-42,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
        "C, A = SDPA\n",
        "\n",
        "print('Masks',attenM[0][0])\n",
        "print()\n",
        "print('nnAttention Scores after softmax: ', A[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hUX_eM_E1B8p"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs_xOAZy3pay",
        "outputId": "6dd58dd1-581c-4b70-e7a1-7febd1ea0241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0700, 0.0592, 0.0682, 0.0480, 0.0831, 0.0651, 0.0727, 0.0426, 0.0636,\n",
              "         0.0688, 0.0676, 0.0488, 0.0392, 0.0364, 0.0296, 0.0500, 0.0488, 0.0384,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0800, 0.0513, 0.0467, 0.0434, 0.0409, 0.0546, 0.0620, 0.0465, 0.0598,\n",
              "         0.0557, 0.0572, 0.0495, 0.0760, 0.0679, 0.0578, 0.0599, 0.0450, 0.0459,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0483, 0.0339, 0.0574, 0.0471, 0.0475, 0.0760, 0.0669, 0.0454, 0.0702,\n",
              "         0.0543, 0.0936, 0.0745, 0.0420, 0.0396, 0.0450, 0.0637, 0.0406, 0.0538,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0507, 0.0534, 0.0739, 0.0597, 0.0445, 0.0463, 0.0600, 0.0526, 0.0671,\n",
              "         0.0676, 0.0529, 0.0496, 0.0527, 0.0444, 0.0579, 0.0447, 0.0539, 0.0681,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0611, 0.0544, 0.0510, 0.0718, 0.0594, 0.0543, 0.0554, 0.0459, 0.0538,\n",
              "         0.0763, 0.0596, 0.0350, 0.0273, 0.0352, 0.0411, 0.0770, 0.0567, 0.0847,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0563, 0.0464, 0.0559, 0.0436, 0.0730, 0.0699, 0.0627, 0.0522, 0.0566,\n",
              "         0.0635, 0.0834, 0.0550, 0.0279, 0.0609, 0.0416, 0.0726, 0.0440, 0.0345,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0643, 0.0750, 0.0767, 0.0571, 0.0707, 0.0598, 0.0772, 0.0576, 0.0634,\n",
              "         0.0668, 0.0540, 0.0320, 0.0351, 0.0440, 0.0338, 0.0383, 0.0385, 0.0557,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0608, 0.0690, 0.0726, 0.0577, 0.0672, 0.0846, 0.0941, 0.0362, 0.0613,\n",
              "         0.0486, 0.0632, 0.0401, 0.0457, 0.0325, 0.0534, 0.0394, 0.0404, 0.0331,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0492, 0.0618, 0.0455, 0.0537, 0.0461, 0.0504, 0.0495, 0.0488, 0.0454,\n",
              "         0.0351, 0.0761, 0.0586, 0.0465, 0.0581, 0.0635, 0.0922, 0.0497, 0.0697,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0662, 0.0570, 0.0580, 0.0711, 0.0345, 0.0445, 0.0698, 0.0649, 0.0622,\n",
              "         0.0622, 0.0538, 0.0518, 0.0424, 0.0455, 0.0462, 0.0810, 0.0407, 0.0482,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0249, 0.0533, 0.0705, 0.0839, 0.0614, 0.0532, 0.0594, 0.0600, 0.0697,\n",
              "         0.0497, 0.0346, 0.0499, 0.0437, 0.0533, 0.0505, 0.0683, 0.0564, 0.0574,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0527, 0.0465, 0.0527, 0.0896, 0.0455, 0.0532, 0.0621, 0.0871, 0.0577,\n",
              "         0.0480, 0.0780, 0.0298, 0.0555, 0.0635, 0.0405, 0.0524, 0.0479, 0.0374,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0431, 0.0381, 0.0506, 0.0367, 0.0466, 0.0491, 0.0512, 0.0413, 0.0629,\n",
              "         0.0482, 0.0885, 0.0805, 0.0440, 0.0604, 0.0610, 0.0728, 0.0685, 0.0564,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0446, 0.0517, 0.0477, 0.0696, 0.0510, 0.0417, 0.0781, 0.0421, 0.0664,\n",
              "         0.0862, 0.0696, 0.0319, 0.0420, 0.0427, 0.0673, 0.0479, 0.0554, 0.0642,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0418, 0.0360, 0.0440, 0.0599, 0.0318, 0.0351, 0.0523, 0.0530, 0.0402,\n",
              "         0.0615, 0.0759, 0.0631, 0.0534, 0.0802, 0.0575, 0.0818, 0.0565, 0.0760,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0585, 0.0539, 0.0506, 0.0767, 0.0567, 0.0541, 0.0515, 0.0695, 0.0837,\n",
              "         0.0793, 0.0386, 0.0306, 0.0340, 0.0497, 0.0507, 0.0469, 0.0612, 0.0537,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0665, 0.0506, 0.0567, 0.0867, 0.0569, 0.0693, 0.0763, 0.0634, 0.1006,\n",
              "         0.0637, 0.0470, 0.0269, 0.0369, 0.0458, 0.0479, 0.0376, 0.0283, 0.0389,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0556, 0.0433, 0.0426, 0.0536, 0.0275, 0.0368, 0.0538, 0.0592, 0.0815,\n",
              "         0.0657, 0.0788, 0.0447, 0.0445, 0.0497, 0.0630, 0.0916, 0.0488, 0.0592,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0407, 0.0385, 0.0617, 0.0491, 0.0491, 0.0595, 0.0423, 0.0392, 0.0482,\n",
              "         0.0557, 0.0801, 0.0625, 0.0690, 0.0684, 0.0626, 0.0443, 0.0504, 0.0786,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0661, 0.0656, 0.0617, 0.0624, 0.0576, 0.0587, 0.0621, 0.0568, 0.0451,\n",
              "         0.0665, 0.0583, 0.0427, 0.0457, 0.0432, 0.0575, 0.0474, 0.0481, 0.0544,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0518, 0.0582, 0.0622, 0.0517, 0.0490, 0.0572, 0.0634, 0.0386, 0.0350,\n",
              "         0.0601, 0.0830, 0.0507, 0.0708, 0.0590, 0.0542, 0.0396, 0.0431, 0.0725,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0461, 0.0453, 0.0491, 0.0519, 0.0460, 0.0502, 0.0613, 0.0461, 0.0515,\n",
              "         0.0442, 0.1027, 0.0595, 0.0546, 0.0509, 0.0493, 0.0518, 0.0665, 0.0730,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0608, 0.0653, 0.0524, 0.0519, 0.0521, 0.0487, 0.0670, 0.0597, 0.0483,\n",
              "         0.0687, 0.0753, 0.0388, 0.0587, 0.0386, 0.0479, 0.0336, 0.0536, 0.0786,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0546, 0.0484, 0.0562, 0.0467, 0.0577, 0.0572, 0.0746, 0.0426, 0.0417,\n",
              "         0.0718, 0.0828, 0.0718, 0.0546, 0.0460, 0.0461, 0.0353, 0.0491, 0.0628,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0477, 0.0525, 0.0620, 0.0377, 0.0585, 0.0511, 0.0734, 0.0376, 0.0513,\n",
              "         0.0678, 0.0760, 0.0453, 0.0551, 0.0510, 0.0683, 0.0432, 0.0564, 0.0652,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0699, 0.0387, 0.0434, 0.0351, 0.0426, 0.0551, 0.0679, 0.0370, 0.0475,\n",
              "         0.0635, 0.1236, 0.0511, 0.0626, 0.0406, 0.0569, 0.0386, 0.0551, 0.0708,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0563, 0.0588, 0.0480, 0.0522, 0.0476, 0.0565, 0.0597, 0.0447, 0.0449,\n",
              "         0.0565, 0.0784, 0.0522, 0.0680, 0.0535, 0.0674, 0.0399, 0.0416, 0.0738,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0613, 0.0522, 0.0507, 0.0488, 0.0510, 0.0560, 0.0484, 0.0350, 0.0416,\n",
              "         0.0534, 0.0953, 0.0480, 0.0663, 0.0523, 0.0748, 0.0437, 0.0467, 0.0743,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0519, 0.0473, 0.0573, 0.0461, 0.0554, 0.0588, 0.0697, 0.0378, 0.0615,\n",
              "         0.0646, 0.0688, 0.0498, 0.0863, 0.0356, 0.0559, 0.0264, 0.0502, 0.0766,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0747, 0.0462, 0.0584, 0.0500, 0.0463, 0.0659, 0.0645, 0.0426, 0.0619,\n",
              "         0.0705, 0.0695, 0.0474, 0.0595, 0.0377, 0.0659, 0.0374, 0.0478, 0.0538,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "MHA= MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "Output, A = MHA\n",
        "\n",
        "A[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "E3YcjYxljvPU",
        "outputId": "e5036c97-f43f-4ecd-d3e0-2e887741ef64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(A[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_GQFL_Va4N4Y"
      },
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RgmfjTqw4Qnw"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "s4p0ulydjvPV"
      },
      "source": [
        "### 7. BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_model, d_model]\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QIQYC1KkjvPW"
      },
      "source": [
        "### 8. Train Our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "f2aea3c6-ee79-4d1c-f415-6c12f0445679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0010 cost = 152.215408\n",
            "Epoch: 0020 cost = 26.173033\n",
            "Epoch: 0030 cost = 16.047791\n",
            "Epoch: 0040 cost = 9.183825\n",
            "Epoch: 0050 cost = 5.328363\n",
            "Epoch: 0060 cost = 4.378877\n",
            "Epoch: 0070 cost = 3.558329\n",
            "Epoch: 0080 cost = 2.709712\n",
            "Epoch: 0090 cost = 2.611641\n",
            "Epoch: 0100 cost = 2.553239\n",
            "Epoch: 0110 cost = 2.575257\n",
            "Epoch: 0120 cost = 2.496284\n",
            "Epoch: 0130 cost = 2.577136\n",
            "Epoch: 0140 cost = 2.516049\n",
            "Epoch: 0150 cost = 2.566643\n",
            "Epoch: 0160 cost = 2.500648\n",
            "Epoch: 0170 cost = 2.576819\n",
            "Epoch: 0180 cost = 2.489408\n",
            "Epoch: 0190 cost = 2.458517\n",
            "Epoch: 0200 cost = 2.493874\n",
            "Epoch: 0210 cost = 2.515061\n",
            "Epoch: 0220 cost = 2.434853\n",
            "Epoch: 0230 cost = 2.478426\n",
            "Epoch: 0240 cost = 2.450401\n",
            "Epoch: 0250 cost = 2.410520\n",
            "Epoch: 0260 cost = 2.408782\n",
            "Epoch: 0270 cost = 2.464524\n",
            "Epoch: 0280 cost = 2.284095\n",
            "Epoch: 0290 cost = 2.049003\n",
            "Epoch: 0300 cost = 2.340172\n",
            "Epoch: 0310 cost = 2.090949\n",
            "Epoch: 0320 cost = 1.816317\n",
            "Epoch: 0330 cost = 1.869203\n",
            "Epoch: 0340 cost = 1.804448\n",
            "Epoch: 0350 cost = 2.099547\n",
            "Epoch: 0360 cost = 1.765623\n",
            "Epoch: 0370 cost = 1.841153\n",
            "Epoch: 0380 cost = 1.802431\n",
            "Epoch: 0390 cost = 2.024388\n",
            "Epoch: 0400 cost = 3.617305\n",
            "Epoch: 0410 cost = 2.022427\n",
            "Epoch: 0420 cost = 2.867967\n",
            "Epoch: 0430 cost = 1.921684\n",
            "Epoch: 0440 cost = 1.889215\n",
            "Epoch: 0450 cost = 1.939721\n",
            "Epoch: 0460 cost = 2.346293\n",
            "Epoch: 0470 cost = 1.942149\n",
            "Epoch: 0480 cost = 1.828012\n",
            "Epoch: 0490 cost = 1.831853\n",
            "Epoch: 0500 cost = 2.090127\n"
          ]
        }
      ],
      "source": [
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "hBedN3CKjvPW"
      },
      "source": [
        "### 9. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "086d3bb0-18b1-47f5-91b4-46d783d6399a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? I am Ali.\n",
            "Hello, Ali My name is wael. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, wael\n",
            "Thank you Ali\n",
            "['[CLS]', 'great', 'my', 'baseball', 'team', '[MASK]', 'the', 'competition', '[SEP]', 'hello', 'how', 'are', '[MASK]', 'i', '[MASK]', 'ali', '[SEP]']\n",
            "masked tokens list :  [12, 5, 23]\n",
            "predict masked tokens list :  [25, 25, 25]\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "print(text)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Zf97uJJS4grJ"
      },
      "outputs": [],
      "source": [
        "not good in MLM >>> need more epochs\n",
        "but good in NSP"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}